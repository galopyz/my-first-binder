{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fdfc5d",
   "metadata": {},
   "source": [
    "# Huggingface Basic usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632c806",
   "metadata": {},
   "source": [
    "## Behind the pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e7467",
   "metadata": {},
   "source": [
    "First, grab a tokenizer with AutoTokenizer. The only thing that tokenizer does is transforming a string input into numbers so that computers can understand. Each model has its own tokenizer that it was trained with, so we have to grab the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fc288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c894892",
   "metadata": {},
   "source": [
    "Now that we have the right tokenizer, we turn our raw input into tensors. The following shows how to turn raw_inputs into pytorch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf87da51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96090f3",
   "metadata": {},
   "source": [
    "<code>return_tensors</code> parameter can be changed so that the return type can be changed. With \"np\", it returns numpy ndarray. If it is \"tf\", it is tensorflow tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136d0ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"np\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ad6e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f228cd",
   "metadata": {},
   "source": [
    "We can grab models by using Automodel and following the same steps as using a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e30f6521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4f95cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ad9b6",
   "metadata": {},
   "source": [
    "However, what we want is classification model, which we get automatically from AutoModelForSequeceClassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19703be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "736baeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a184dd",
   "metadata": {},
   "source": [
    "Logits are raw outputs from passing our inputs into the model. To make it more useful, we need to turn it into percentages. That's what softmax is for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "823d08f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77caa8b",
   "metadata": {},
   "source": [
    "Each tensor sums up to 1 because that's what probability sums up to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f12bf5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ee193",
   "metadata": {},
   "source": [
    "In case we forget which is negative or positive, we can look it up this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3bb35c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757e789",
   "metadata": {},
   "source": [
    "Here are some examples going through each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96bf3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab raw inputs\n",
    "raw_inputs = [\n",
    "    \"All I want is freedom.\",\n",
    "    \"I wish I can eat some chicken nuggets.\",\n",
    "    \"If I enjoy the food, I don't get fat.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fd3a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a tokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f9cb308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2035,  1045,  2215,  2003,  4071,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  4299,  1045,  2064,  4521,  2070,  7975, 16371, 13871,\n",
      "          8454,  1012,   102,     0,     0],\n",
      "        [  101,  2065,  1045,  5959,  1996,  2833,  1010,  1045,  2123,  1005,\n",
      "          1056,  2131,  6638,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Turn raw inputs into inputs \n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0c02b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6301,  2.7617],\n",
       "        [ 3.0354, -2.5513],\n",
       "        [-3.4392,  3.5611]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a classification model to get the outputs\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04b80f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5329e-03, 9.9547e-01],\n",
      "        [9.9627e-01, 3.7333e-03],\n",
      "        [9.1076e-04, 9.9909e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Turn our outputs into percentages\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03f3ebda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case we forget which is positive\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37886198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
