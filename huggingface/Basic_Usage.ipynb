{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fdfc5d",
   "metadata": {},
   "source": [
    "# Huggingface Basic usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d59e8",
   "metadata": {},
   "source": [
    "Just having fun with huggingface course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632c806",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Behind the pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e7467",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, grab a tokenizer with AutoTokenizer. The only thing that tokenizer does is transforming a string input into numbers so that computers can understand. Each model has its own tokenizer that it was trained with, so we have to grab the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fc288e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c894892",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have the right tokenizer, we turn our raw input into tensors. The following shows how to turn raw_inputs into pytorch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf87da51",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96090f3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<code>return_tensors</code> parameter can be changed so that the return type can be changed. With \"np\", it returns numpy ndarray. If it is \"tf\", it is tensorflow tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136d0ab2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"np\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ad6e1e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f228cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can grab models by using Automodel and following the same steps as using a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e30f6521",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4f95cf5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ad9b6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, what we want is classification model, which we get automatically from AutoModelForSequeceClassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19703be5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "736baeaf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a184dd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Logits are raw outputs from passing our inputs into the model. To make it more useful, we need to turn it into percentages. That's what softmax is for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "823d08f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77caa8b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each tensor sums up to 1 because that's what probability sums up to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f12bf5f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ee193",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In case we forget which is negative or positive, we can look it up this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3bb35c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757e789",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are some examples going through each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96bf3a88",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Grab raw inputs\n",
    "raw_inputs = [\n",
    "    \"All I want is freedom.\",\n",
    "    \"I wish I can eat some chicken nuggets.\",\n",
    "    \"If I enjoy the food, I don't get fat.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fd3a19b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get a tokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f9cb308",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2035,  1045,  2215,  2003,  4071,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  4299,  1045,  2064,  4521,  2070,  7975, 16371, 13871,\n",
      "          8454,  1012,   102,     0,     0],\n",
      "        [  101,  2065,  1045,  5959,  1996,  2833,  1010,  1045,  2123,  1005,\n",
      "          1056,  2131,  6638,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Turn raw inputs into inputs \n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0c02b4c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6301,  2.7617],\n",
       "        [ 3.0354, -2.5513],\n",
       "        [-3.4392,  3.5611]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a classification model to get the outputs\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04b80f9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5329e-03, 9.9547e-01],\n",
      "        [9.9627e-01, 3.7333e-03],\n",
      "        [9.1076e-04, 9.9909e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Turn our outputs into percentages\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03f3ebda",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case we forget which is positive\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1340192",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131de9d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are many ways to use the models from huggingface. One way is getting the model by using its config. Config has all the information necessary to understand the model, and model has all the parameters. For example, model is a treasure chest and config is a key. We need both in order to get access to the treasure. However, this model is not ready to use yet because it needs to be trained. Model parameters are randomly initialized, which means there is no treasure inside of the treasure chest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb0253f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e91277",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.18.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dc686",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another way to get a model is using pretrained model. This model has already been trained with data, computing power, and time. It is ready to use, and full of treasure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e69d9858",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593a0c0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Models can be saved to a local directory. When saved, there are config file and model file as we built the model the first time. Their purpose is the same. config file has all the configurations needed to make the model, and model file has the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da3f9c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9e0097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48850af",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 423164\r\n",
      "-rw-rw-r-- 1 galopy galopy       656 May 13 12:54 config.json\r\n",
      "-rw-rw-r-- 1 galopy galopy 433307697 May 13 12:54 pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f75f1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is an example of how to use a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edc266fd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Grab a sentence\n",
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14226de5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encode it using tokenizer\n",
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9181cea7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Turn it into tensors\n",
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "875acf2b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.4496e-01,  4.8276e-01,  2.7797e-01,  ..., -5.4032e-02,\n",
       "           3.9393e-01, -9.4770e-02],\n",
       "         [ 2.4943e-01, -4.4093e-01,  8.1772e-01,  ..., -3.1917e-01,\n",
       "           2.2992e-01, -4.1172e-02],\n",
       "         [ 1.3668e-01,  2.2518e-01,  1.4502e-01,  ..., -4.6915e-02,\n",
       "           2.8224e-01,  7.5566e-02],\n",
       "         [ 1.1789e+00,  1.6739e-01, -1.8187e-01,  ...,  2.4671e-01,\n",
       "           1.0441e+00, -6.1970e-03]],\n",
       "\n",
       "        [[ 3.6436e-01,  3.2465e-02,  2.0258e-01,  ...,  6.0111e-02,\n",
       "           3.2451e-01, -2.0995e-02],\n",
       "         [ 7.1866e-01, -4.8725e-01,  5.1740e-01,  ..., -4.4012e-01,\n",
       "           1.4553e-01, -3.7545e-02],\n",
       "         [ 3.3223e-01, -2.3271e-01,  9.4876e-02,  ..., -2.5268e-01,\n",
       "           3.2172e-01,  8.1096e-04],\n",
       "         [ 1.2523e+00,  3.5754e-01, -5.1320e-02,  ..., -3.7840e-01,\n",
       "           1.0526e+00, -5.6255e-01]],\n",
       "\n",
       "        [[ 2.4042e-01,  1.4718e-01,  1.2110e-01,  ...,  7.6062e-02,\n",
       "           3.3564e-01,  2.8262e-01],\n",
       "         [ 6.5701e-01, -3.2787e-01,  2.4968e-01,  ..., -2.5920e-01,\n",
       "           2.0175e-01,  3.3275e-01],\n",
       "         [ 2.0160e-01,  1.5783e-01,  9.8971e-03,  ..., -3.8850e-01,\n",
       "           4.1308e-01,  3.9732e-01],\n",
       "         [ 1.0175e+00,  6.4387e-01, -7.8147e-01,  ..., -4.2109e-01,\n",
       "           1.0925e+00, -4.8456e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6856,  0.5262,  1.0000,  ...,  1.0000, -0.6112,  0.9971],\n",
       "        [-0.6055,  0.4997,  0.9998,  ...,  0.9999, -0.6753,  0.9769],\n",
       "        [-0.7702,  0.5447,  0.9999,  ...,  1.0000, -0.4655,  0.9894]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use model on it.\n",
    "output = model(model_inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c27cc4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d4e7ed",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tokenizing is important because it can either make the model easy to train or not. Even though the neural network is flexible and adaptable, it may struggle to fit. Also, it takes computing time and electricity to train, and it is best to use least resource possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19aeba6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can grab tokenizer with either specific model's tokenizer or AutoTokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a681b155",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8767bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43106f00",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we can use it and save it if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82944d26",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e445ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/tokenizer_config.json',\n",
       " 'models/special_tokens_map.json',\n",
       " 'models/vocab.txt',\n",
       " 'models/added_tokens.json',\n",
       " 'models/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03a01a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is how to use the tokenizer to turn it into tokens. Then, by using tokenizer.convert_tokens_to_ids, we get the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa87a4d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40db5022",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7993, 170, 13809, 23763, 2443, 1110, 3014]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a68b0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Changing back to string is also easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d18a0a90",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using a transformer network is simple'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "decoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a662a",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
